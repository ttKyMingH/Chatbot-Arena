{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8820093,"sourceType":"datasetVersion","datasetId":5306138},{"sourceId":8826860,"sourceType":"datasetVersion","datasetId":5310688},{"sourceId":8826894,"sourceType":"datasetVersion","datasetId":5310716},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"jax\" or \"torch\"\nimport re\n\nimport keras_nlp\nimport keras\nimport tensorflow as tf\n\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport plotly.express as px","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-01T05:42:43.829095Z","iopub.execute_input":"2024-07-01T05:42:43.829460Z","iopub.status.idle":"2024-07-01T05:42:43.836148Z","shell.execute_reply.started":"2024-07-01T05:42:43.829433Z","shell.execute_reply":"2024-07-01T05:42:43.835174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Num GPUs Available","metadata":{}},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:43.838432Z","iopub.execute_input":"2024-07-01T05:42:43.838706Z","iopub.status.idle":"2024-07-01T05:42:43.852598Z","shell.execute_reply.started":"2024-07-01T05:42:43.838682Z","shell.execute_reply":"2024-07-01T05:42:43.851585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 42  # Random seed\n    preset = \"deberta_v3_extra_small_en\"\n    sequence_length = 512\n    epochs = 6\n    batch_size = 16\n    scheduler = 'cosine'  # Learning rate scheduler\n    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n    name2label = {v:k for k, v in label2name.items()}\n    class_labels = list(label2name.keys())\n    class_names = list(label2name.values())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-01T05:42:43.854194Z","iopub.execute_input":"2024-07-01T05:42:43.854542Z","iopub.status.idle":"2024-07-01T05:42:43.862088Z","shell.execute_reply.started":"2024-07-01T05:42:43.854510Z","shell.execute_reply":"2024-07-01T05:42:43.860936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:43.863313Z","iopub.execute_input":"2024-07-01T05:42:43.863625Z","iopub.status.idle":"2024-07-01T05:42:43.875207Z","shell.execute_reply.started":"2024-07-01T05:42:43.863597Z","shell.execute_reply":"2024-07-01T05:42:43.874296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Mixed Precision\n\nIn this notebook, we will use mixed precision instead of float32 precision for training and inference to reduce GPU memory usage. This will ultimately allow us to use larger batch sizes, thus reducing our training and inference time.","metadata":{}},{"cell_type":"code","source":"keras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:43.877720Z","iopub.execute_input":"2024-07-01T05:42:43.878053Z","iopub.status.idle":"2024-07-01T05:42:43.884289Z","shell.execute_reply.started":"2024-07-01T05:42:43.878025Z","shell.execute_reply":"2024-07-01T05:42:43.883335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Path ","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:43.885662Z","iopub.execute_input":"2024-07-01T05:42:43.886391Z","iopub.status.idle":"2024-07-01T05:42:43.894550Z","shell.execute_reply.started":"2024-07-01T05:42:43.886354Z","shell.execute_reply":"2024-07-01T05:42:43.893533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta Data \n## Files\n\n### `train.csv`\n- `id`: Unique identifier for each row.\n- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).\n\n### `test.csv`\n- `id`: Unique identifier for each row.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n\n> Note that each interaction may have multiple prompts and responses, but this notebook will use only **one prompt per interaction**. You can choose to use all prompts and responses. Additionally, prompts and responses in the dataframe are provided as string-formatted lists, so they need to be converted to literal lists using `eval()`.\n","metadata":{}},{"cell_type":"code","source":"# Load Train Data\ndf = pd.read_csv(f'{BASE_PATH}/train.csv') \nultrachat_df = pd.read_csv('/kaggle/input/ultrachat-train/ultrachat_s42_a0.5.csv')\ndf = pd.concat([df, ultrachat_df], axis=0)\nlmsys_33k_deduplicated = pd.read_csv('/kaggle/input/lmsys-33k-deduplicated/lmsys-33k-deduplicated.csv')\ndf = pd.concat([df, lmsys_33k_deduplicated], axis=0)\n# ultrafeedback_lmsysformat = pd.read_parquet('/kaggle/input/ultrafeedback-lmsysformat/ultrafeedback_lmsysformat.parquet', engine='pyarrow')\n# ultrafeedback_lmsysformat['prompt'] = ultrafeedback_lmsysformat['prompt'].apply(lambda x: f'[\"{x}\"]')\n# df = pd.concat([df, ultrafeedback_lmsysformat], axis=0)\n\n# Load Test Data\ntest_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n\n# display(ultrafeedback_lmsysformat.head())\ndisplay(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:43.895860Z","iopub.execute_input":"2024-07-01T05:42:43.896494Z","iopub.status.idle":"2024-07-01T05:42:47.570972Z","shell.execute_reply.started":"2024-07-01T05:42:43.896469Z","shell.execute_reply":"2024-07-01T05:42:47.570076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(\"id\", axis=1)\ndf = df.drop_duplicates(keep=\"first\", ignore_index=True)\n\nfor col in [\"prompt\"]:\n    df[col] = df[col].apply(lambda x: eval(x))\n    test_df[col] = test_df[col].apply(lambda x: eval(x))\nfor col in [\"response_a\", \"response_b\"]:\n    df[col] = df[col].apply(lambda x: eval(x.replace(\"null\", \"None\")))\n    test_df[col] = test_df[col].apply(lambda x: eval(x.replace(\"null\", \"None\")))\n    \n# Sample data\n# df = df.sample(frac=0.01)\n\n# Label conversion\ndf[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\ndf[\"class_label\"] = df.class_name.map(CFG.name2label)\n\n# Show Sample\ndisplay(df.head())\n# Show Sample\ndisplay(test_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:47.572197Z","iopub.execute_input":"2024-07-01T05:42:47.572495Z","iopub.status.idle":"2024-07-01T05:42:56.981935Z","shell.execute_reply.started":"2024-07-01T05:42:47.572468Z","shell.execute_reply":"2024-07-01T05:42:56.981115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Contextualize Response with Prompt\n\nIn our approach, we will contextualize each response with the prompt instead of using a single prompt for all responses. This means that for each response, we will provide the model with the same set of prompts combined with their respective response (e.g., `(P + R_A)`, `(P + R_B)`, etc.). This approach is similar to the multiple-choice question task in NLP.\n\n> Note that some prompts and responses may not be encoded with `utf-8`, resulting in errors when creating the dataloader. In such cases, we will replace them with an empty string.\n","metadata":{}},{"cell_type":"code","source":"def make_pairs(row):\n    row['options'] = []\n    row[\"encode_fail\"] = False\n\n    try:\n        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„é”®éƒ½å­˜åœ¨äºrowå­—å…¸ä¸­\n        prompts = row['prompt']\n        responses_a = row['response_a']\n        responses_b = row['response_b']\n        \n        # æ£€æŸ¥åˆ—è¡¨é•¿åº¦æ˜¯å¦åŒ¹é…\n        if not (len(prompts) == len(responses_a) == len(responses_b)):\n            raise ValueError(\"The lists 'prompt', 'response_a', and 'response_b' must be of the same length.\")\n            \n        response_a_str = ''\n        response_b_str = ''\n        \n        for idx in range(len(prompts)):\n            response_a_str += f\"Prompt: {prompts[idx]}\\n\\nResponse: {responses_a[idx]}\"\n            response_b_str += f\"Prompt: {prompts[idx]}\\n\\nResponse: {responses_b[idx]}\"\n        \n        # æ–‡æœ¬æ¸…æ´—ï¼Œä¾‹å¦‚å»é™¤æ— æ³•è¯†åˆ«çš„Unicodeå­—ç¬¦æˆ–æ›¿æ¢å®ƒä»¬\n        clean_response_a_str = \"\".join(filter(lambda x: ord(x) < 128, response_a_str))\n        clean_response_a_str = \"\".join(filter(lambda x: ord(x) < 128, response_b_str))\n        \n        row['options'].append(clean_response_a_str)\n        row['options'].append(clean_response_a_str)\n        \n    except KeyError as e:\n        print(f\"Missing key in row: {e}\")\n        row[\"encode_fail\"] = True\n    except ValueError as e:\n        print(e)\n        row[\"encode_fail\"] = True\n    except Exception as e:\n        # æ•è·å…¶ä»–æ‰€æœ‰å¼‚å¸¸\n        print(f\"An unexpected error occurred: {e}\")\n        row[\"encode_fail\"] = True\n\n    return row","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:56.990688Z","iopub.execute_input":"2024-07-01T05:42:56.990990Z","iopub.status.idle":"2024-07-01T05:42:57.002862Z","shell.execute_reply.started":"2024-07-01T05:42:56.990964Z","shell.execute_reply":"2024-07-01T05:42:57.001924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.apply(make_pairs, axis=1)\ndisplay(df.head(2))\n\ntest_df = test_df.apply(make_pairs, axis=1)\ndisplay(test_df.head(2))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:42:57.004172Z","iopub.execute_input":"2024-07-01T05:42:57.004618Z","iopub.status.idle":"2024-07-01T05:46:00.842076Z","shell.execute_reply.started":"2024-07-01T05:42:57.004586Z","shell.execute_reply":"2024-07-01T05:46:00.841150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding Fail Statistics\n\nLet's examine how many samples have encoding issues. From the code below, we can see that only $1\\%$ of the samples failed to be encoded, while $99\\%$ of the samples don't have any issues. A similar pattern can be expected for the test data as well. Thus, considering empty strings for this small portion of the data will not have much impact on our training and inference.","metadata":{}},{"cell_type":"code","source":"df.encode_fail.value_counts(normalize=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:00.843269Z","iopub.execute_input":"2024-07-01T05:46:00.843559Z","iopub.status.idle":"2024-07-01T05:46:00.855046Z","shell.execute_reply.started":"2024-07-01T05:46:00.843533Z","shell.execute_reply":"2024-07-01T05:46:00.854120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"class DataFrameStatsProcessor:\n    def __init__(self, df):\n        self.df = df\n\n    def _is_empty(self, string: str) -> bool:\n        return bool(re.match(\"^\\s*$\", string))\n\n    def _len(self, string: str) -> int:\n        if string is None:\n            return 0\n        return len(string)\n\n    def _add_len_stats(self, col: str) -> pd.DataFrame:\n        if col == \"prompt\":\n            col_prefix = \"p_len\"\n        elif col == \"response_a\":\n            col_prefix = \"res_a_len\"\n        elif col == \"response_b\":\n            col_prefix = \"res_b_len\"\n        \n        self.df[f\"{col_prefix}_sum\"] = self.df[col].apply(lambda x: sum(self._len(s) for s in x))\n        self.df[f\"{col_prefix}_mean\"] =  self.df[col].apply(lambda x: np.mean(list(self._len(s) for s in x)))\n        self.df[f\"{col_prefix}_max\"] = self.df[col].apply(lambda x: max(self._len(s) for s in x))\n        self.df[f\"{col_prefix}_sum_log\"] = np.log1p(self.df[f\"{col_prefix}_sum\"])\n        self.df[f\"{col_prefix}_mean_log\"] =  np.log1p(self.df[f\"{col_prefix}_mean\"])\n        self.df[f\"{col_prefix}_max_log\"] = np.log1p(self.df[f\"{col_prefix}_max\"])\n        \n        return self.df\n    \n    def z_score_normalize(self, columns):\n        \"\"\"\n        å¯¹æŒ‡å®šçš„åˆ—è¿›è¡ŒZå¾—åˆ†å½’ä¸€åŒ–ã€‚\n        å‚æ•°:\n            columns (list): éœ€è¦è¿›è¡ŒZå¾—åˆ†å½’ä¸€åŒ–çš„åˆ—ååˆ—è¡¨ã€‚\n        \"\"\"\n        for col in columns:\n            self.df[col] = (self.df[col] - self.df[col].mean()) / self.df[col].std()\n    \n    def process_dataframe(self):\n        self.df[\"n_prompts\"] = self.df[\"prompt\"].apply(lambda x: len(x))\n        self.df[\"n_res_a\"] = self.df[\"response_a\"].apply(lambda x: len(x))\n        self.df[\"n_res_b\"] = self.df[\"response_b\"].apply(lambda x: len(x))\n        assert ((self.df[\"n_prompts\"] == self.df[\"n_res_a\"]) & (self.df[\"n_prompts\"] == self.df[\"n_res_b\"])).all()\n\n        self.df[\"n_na_prompts\"] = self.df[\"prompt\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n        self.df[\"n_empty_prompts\"] = self.df[\"prompt\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n        self.df[\"n_na_res_a\"] = self.df[\"response_a\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n        self.df[\"n_empty_res_a\"] = self.df[\"response_a\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n        self.df[\"n_na_res_b\"] = self.df[\"response_b\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n        self.df[\"n_empty_res_b\"] = self.df[\"response_b\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n\n        self.df[\"n_miss_res_a\"] = self.df[\"n_na_res_a\"] + self.df[\"n_empty_res_a\"]\n        self.df[\"n_miss_res_b\"] = self.df[\"n_na_res_b\"] + self.df[\"n_empty_res_b\"]\n\n        self.df[\"n_eff_res_a\"] = self.df[\"n_res_a\"] - self.df[\"n_miss_res_a\"]\n        self.df[\"n_eff_res_b\"] = self.df[\"n_res_b\"] - self.df[\"n_miss_res_b\"]\n\n        self._add_len_stats(\"prompt\")\n        self._add_len_stats(\"response_a\")\n        self._add_len_stats(\"response_b\")\n\n        self.df[\"res_len_mean_diff\"] = self.df[\"res_a_len_mean\"] - self.df[\"res_b_len_mean\"]\n        self.df[\"res_len_mean_diff_clip\"] = self.df[\"res_len_mean_diff\"].clip(-6000, 6000)\n\n        self.df[\"n_miss_prompts\"] = self.df[\"n_na_prompts\"] + self.df[\"n_empty_prompts\"]\n        self.df[\"n_eff_prompts\"] = self.df[\"n_prompts\"] - self.df[\"n_miss_prompts\"]\n\n        self.df[\"na_prompt_ratio\"] = self.df[\"n_na_prompts\"] / self.df[\"n_prompts\"]\n        self.df[\"empty_prompt_ratio\"] = self.df[\"n_empty_prompts\"] / self.df[\"n_prompts\"]\n        self.df[\"miss_prompt_ratio\"] = self.df[\"n_miss_prompts\"] / self.df[\"n_prompts\"]\n\n        self.df[\"na_res_a_ratio\"] = self.df[\"n_na_res_a\"] / self.df[\"n_res_a\"]\n        self.df[\"empty_res_a_ratio\"] = self.df[\"n_empty_res_a\"] / self.df[\"n_res_a\"]\n        self.df[\"miss_res_a_ratio\"] = self.df[\"n_miss_res_a\"] / self.df[\"n_res_a\"]\n        self.df[\"na_res_b_ratio\"] = self.df[\"n_na_res_b\"] / self.df[\"n_res_b\"]\n        self.df[\"empty_res_b_ratio\"] = self.df[\"n_empty_res_b\"] / self.df[\"n_res_b\"]\n        self.df[\"miss_res_b_ratio\"] = self.df[\"n_miss_res_b\"] / self.df[\"n_res_b\"]\n\n        for col, col_prefix in zip([\"prompt\", \"response_a\", \"response_b\"], [\"p_len\", \"res_a_len\", \"res_b_len\"]):\n            self.df[f\"{col_prefix}_med\"] = self.df[col].apply(lambda x: np.median(list(self._len(s) for s in x)))\n            self.df[f\"{col_prefix}_std\"] = self.df[col].apply(lambda x: np.std(list(self._len(s) for s in x)))\n\n        self.df[\"p_len_eff_mean\"] = self.df[\"p_len_sum\"] / self.df[\"n_eff_prompts\"]\n        self.df[\"res_a_len_eff_mean\"] = self.df[\"res_a_len_sum\"] / self.df[\"n_eff_res_a\"]\n        self.df[\"res_b_len_eff_mean\"] = self.df[\"res_b_len_sum\"] / self.df[\"n_eff_res_b\"]\n\n        for stats in [\"sum\", \"mean\", \"max\", \"med\", \"eff_mean\"]:\n            self.df[f\"p_a_{stats}_diff\"] = self.df[f\"p_len_{stats}\"] - self.df[f\"res_a_len_{stats}\"]\n            self.df[f\"p_b_{stats}_diff\"] = self.df[f\"p_len_{stats}\"] - self.df[f\"res_b_len_{stats}\"]\n            self.df[f\"a_b_{stats}_diff\"] = self.df[f\"res_a_len_{stats}\"] - self.df[f\"res_b_len_{stats}\"]\n            \n        len_feature_a_col = [\"res_a_len_sum\",\"res_a_len_mean\",\"res_a_len_max\",\"res_a_len_sum_log\",\"res_a_len_mean_log\",\"res_a_len_max_log\",\n                     \"res_a_len_med\",\"res_a_len_std\",\"res_a_len_eff_mean\",\"p_a_sum_diff\",\"p_a_mean_diff\",\"p_a_max_diff\",\"p_a_med_diff\",\n                     \"p_a_eff_mean_diff\"]\n        \n        len_feature_b_col = [\"res_b_len_sum\",\"res_b_len_mean\",\"res_b_len_max\",\"res_b_len_sum_log\",\"res_b_len_mean_log\",\"res_b_len_max_log\",\n                             \"res_b_len_med\",\"res_b_len_std\",\"res_b_len_eff_mean\",\"p_b_sum_diff\",\"p_b_mean_diff\",\"p_b_max_diff\",\"p_b_med_diff\",\n                             \"p_b_eff_mean_diff\"]\n        \n        numerical_feature_columns = [\"res_a_len_sum\",\"res_a_len_mean\",\"res_a_len_max\",\"res_a_len_sum_log\",\"res_a_len_mean_log\",\"res_a_len_max_log\",\n                                     \"res_a_len_med\",\"res_a_len_std\",\"res_a_len_eff_mean\",\"p_a_sum_diff\",\"p_a_mean_diff\",\"p_a_max_diff\",\"p_a_med_diff\",\n                                     \"p_a_eff_mean_diff\", \"res_b_len_sum\",\"res_b_len_mean\",\"res_b_len_max\",\"res_b_len_sum_log\",\"res_b_len_mean_log\",\"res_b_len_max_log\",\n                                     \"res_b_len_med\",\"res_b_len_std\",\"res_b_len_eff_mean\",\"p_b_sum_diff\",\"p_b_mean_diff\",\"p_b_max_diff\",\"p_b_med_diff\",\n                                     \"p_b_eff_mean_diff\"]\n        # ç¡®ä¿ä¸é™¤ä»¥é›¶è¿›è¡Œå½’ä¸€åŒ–\n        for col in numerical_feature_columns:\n            if self.df[col].std() == 0:\n                print(f\"Warning: Standard deviation is zero for column {col}. Skipping normalization.\")\n            else:\n                self.z_score_normalize([col])\n                \n        self.df = self.df.fillna(0)\n        \n        # é€‰æ‹©è¿™äº›åˆ—å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºåˆ—è¡¨\n        len_features_a = self.df[len_feature_a_col].values.tolist()\n        len_features_b = self.df[len_feature_b_col].values.tolist()\n\n        return len_features_a, len_features_b","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:00.856673Z","iopub.execute_input":"2024-07-01T05:46:00.857073Z","iopub.status.idle":"2024-07-01T05:46:00.893205Z","shell.execute_reply.started":"2024-07-01T05:46:00.857040Z","shell.execute_reply":"2024-07-01T05:46:00.892273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Split\n\nIn the code snippet provided below, we will divide the existing data into training and validation using a stratification of `class_label` column.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # Import package\n\ntrain_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"class_label\"])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-01T05:46:00.894577Z","iopub.execute_input":"2024-07-01T05:46:00.895333Z","iopub.status.idle":"2024-07-01T05:46:01.457614Z","shell.execute_reply.started":"2024-07-01T05:46:00.895301Z","shell.execute_reply":"2024-07-01T05:46:01.456724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n\nExplore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)","metadata":{}},{"cell_type":"code","source":"preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n    preset=CFG.preset, \n    sequence_length=CFG.sequence_length, \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:01.459033Z","iopub.execute_input":"2024-07-01T05:46:01.459337Z","iopub.status.idle":"2024-07-01T05:46:06.958895Z","shell.execute_reply.started":"2024-07-01T05:46:01.459300Z","shell.execute_reply":"2024-07-01T05:46:06.957936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method.","metadata":{}},{"cell_type":"code","source":"def preprocess_fn(text, label=None, features_a=None, features_b=None):\n    text = preprocessor(text)\n    if features_a is not None:\n        text['features_a'] = features_a\n    if features_b is not None:\n         text['features_b'] = features_b\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:06.960133Z","iopub.execute_input":"2024-07-01T05:46:06.960423Z","iopub.status.idle":"2024-07-01T05:46:06.965760Z","shell.execute_reply.started":"2024-07-01T05:46:06.960396Z","shell.execute_reply":"2024-07-01T05:46:06.964931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FGM","metadata":{}},{"cell_type":"code","source":"# # æ·»åŠ  FGM æ‰°åŠ¨å‡½æ•°\n# def fgm_perturb(features, epsilon=1.0):\n#     # è®¡ç®—æ‰°åŠ¨é‡ï¼Œepsilon ä¸ºæ‰°åŠ¨æ¯”ä¾‹\n#     perturbation = np.random.uniform(-1, 1, features.shape) * epsilon\n#     # åº”ç”¨æ‰°åŠ¨\n#     return features + perturbation","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:06.967153Z","iopub.execute_input":"2024-07-01T05:46:06.967510Z","iopub.status.idle":"2024-07-01T05:46:06.978818Z","shell.execute_reply.started":"2024-07-01T05:46:06.967479Z","shell.execute_reply":"2024-07-01T05:46:06.978090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ä¿®æ”¹æ•°æ®é¢„å¤„ç†å‡½æ•°ä»¥åŒ…å« FGM æ‰°åŠ¨\n# def preprocess_fn(text, label=None, features_a=None, features_b=None, is_fgm=False, epsilon=1.0):\n#     # é¢„å¤„ç†æ–‡æœ¬\n#     text = preprocessor(text)\n#     if features_a is not None:\n#         if is_fgm:\n#             # å¦‚æœæ˜¯ FGMï¼Œåº”ç”¨æ‰°åŠ¨\n#             features_a = fgm_perturb(features_a, epsilon)\n#         text['features_a'] = features_a\n#     if features_b is not None:\n#         if is_fgm:\n#             # å¦‚æœæ˜¯ FGMï¼Œåº”ç”¨æ‰°åŠ¨\n#             features_b = fgm_perturb(features_b, epsilon)\n#         text['features_b'] = features_b\n#     return (text, label) if label is not None else text","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:06.984351Z","iopub.execute_input":"2024-07-01T05:46:06.984817Z","iopub.status.idle":"2024-07-01T05:46:06.991620Z","shell.execute_reply.started":"2024-07-01T05:46:06.984791Z","shell.execute_reply":"2024-07-01T05:46:06.990902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AWP","metadata":{}},{"cell_type":"code","source":"#å®šä¹‰ AWP æ‰°åŠ¨å‡½æ•°\ndef awp_perturb(model, epsilon=1e-4):\n    for layer in model.layers:\n        if hasattr(layer, 'kernel'):\n            # è·å–æƒé‡\n            weights = layer.kernel\n            # è®¡ç®—æ‰°åŠ¨\n            perturbation = tf.random.normal(weights.shape, stddev=epsilon)\n            # åº”ç”¨æ‰°åŠ¨\n            layer.kernel.assign_add(perturbation)\n\n#åˆ›å»º AWP å›è°ƒå‡½æ•°\nclass AWPCallback(keras.callbacks.Callback):\n    def __init__(self, epsilon):\n        super(AWPCallback, self).__init__()\n        self.epsilon = epsilon\n\n    def on_batch_begin(self, batch, logs=None):\n        # åœ¨æ¯ä¸ªæ‰¹æ¬¡å¼€å§‹æ—¶åº”ç”¨ AWP æ‰°åŠ¨\n        awp_perturb(self.model, self.epsilon)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:06.992834Z","iopub.execute_input":"2024-07-01T05:46:06.993228Z","iopub.status.idle":"2024-07-01T05:46:07.002199Z","shell.execute_reply.started":"2024-07-01T05:46:06.993197Z","shell.execute_reply":"2024-07-01T05:46:07.001366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸš | DataLoader\n\nThe code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.\n\nTo learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).","metadata":{}},{"cell_type":"code","source":"def build_dataset_with_features(texts, labels=None, features_a=None, features_b=None, batch_size=32, is_fgm=False,  epsilon=1.0,\n                                cache=True, shuffle=1024):\n    AUTO = tf.data.AUTOTUNE\n    if (features_a is not None) and (features_b is not None):\n        slices = (texts, None, features_a, features_b) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3), features_a, features_b)  # Create slices\n    else:\n        slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.cache() if cache else ds\n    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)\n#     ds = ds.map(lambda x: preprocess_fn(x, features_a=features_a, features_b=features_b, is_fgm=is_fgm, epsilon=epsilon),\n#                 num_parallel_calls=tf.data.AUTOTUNE)\n    opt = tf.data.Options()\n    if shuffle:\n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)\n    ds = ds.batch(batch_size, drop_remainder=False)\n    ds = ds.prefetch(AUTO)\n    \n    return ds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-01T05:46:07.003226Z","iopub.execute_input":"2024-07-01T05:46:07.003498Z","iopub.status.idle":"2024-07-01T05:46:07.013033Z","shell.execute_reply.started":"2024-07-01T05:46:07.003475Z","shell.execute_reply":"2024-07-01T05:46:07.012260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Train/Valid Dataloader","metadata":{}},{"cell_type":"code","source":"train_features_processor = DataFrameStatsProcessor(train_df.copy())\ntrain_features_a, train_features_b = train_features_processor.process_dataframe()\nvalid_features_processor = DataFrameStatsProcessor(valid_df.copy())\nvalid_features_a, valid_features_b = valid_features_processor.process_dataframe()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:07.014293Z","iopub.execute_input":"2024-07-01T05:46:07.014974Z","iopub.status.idle":"2024-07-01T05:46:37.861252Z","shell.execute_reply.started":"2024-07-01T05:46:07.014935Z","shell.execute_reply":"2024-07-01T05:46:37.860438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train\ntrain_texts = train_df.options.tolist()  \ntrain_labels = train_df.class_label.tolist() \ntrain_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b, \n                         batch_size=CFG.batch_size,\n                         shuffle=True)\n# # Valid\nvalid_texts = valid_df.options.tolist()  \nvalid_labels = valid_df.class_label.tolist() \nvalid_ds = build_dataset_with_features(valid_texts, valid_labels, valid_features_a, valid_features_b, \n                         batch_size=CFG.batch_size,\n                         shuffle=False)\nprint(train_ds)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-07-01T05:46:37.862426Z","iopub.execute_input":"2024-07-01T05:46:37.862761Z","iopub.status.idle":"2024-07-01T05:46:58.426303Z","shell.execute_reply.started":"2024-07-01T05:46:37.862728Z","shell.execute_reply":"2024-07-01T05:46:58.425400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LR Schedule\n\nImplementing a learning rate scheduler is crucial for transfer learning. The learning rate initiates at `lr_start` and gradually tapers down to `lr_min` using various techniques, including:\n- `step`: Lowering the learning rate in step-wise manner resembling stairs.\n- `cos`: Utilizing a cosine curve to gradually reduce the learning rate.\n- `exp`: Exponentially decreasing the learning rate.\n\n**Importance:** A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.","metadata":{}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-01T05:46:58.427518Z","iopub.execute_input":"2024-07-01T05:46:58.427821Z","iopub.status.idle":"2024-07-01T05:46:58.438250Z","shell.execute_reply.started":"2024-07-01T05:46:58.427795Z","shell.execute_reply":"2024-07-01T05:46:58.437393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.batch_size, epochs=CFG.epochs, plot=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:58.439264Z","iopub.execute_input":"2024-07-01T05:46:58.439518Z","iopub.status.idle":"2024-07-01T05:46:58.730319Z","shell.execute_reply.started":"2024-07-01T05:46:58.439496Z","shell.execute_reply":"2024-07-01T05:46:58.729401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Checkpointing\n\nThe following code will create a callback that will save the best checkpoint of the model during training, which we will use for inference in the submission.","metadata":{}},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',\n                                          monitor='val_log_loss',\n                                          save_best_only=True,\n                                          save_weights_only=True,\n                                          mode='min')  # Get Model checkpoint callback","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-01T05:46:58.731607Z","iopub.execute_input":"2024-07-01T05:46:58.731929Z","iopub.status.idle":"2024-07-01T05:46:58.738164Z","shell.execute_reply.started":"2024-07-01T05:46:58.731894Z","shell.execute_reply":"2024-07-01T05:46:58.737257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric\n\nThe metric for this competition is **Log Loss**. This metric can be expressed mathematically as,\n\n$$\n\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)\n$$\n\nwhere $ N $ is the number of samples, $ y_i $ is the true label, and $ p_i $ is the predicted probability of the sample belonging to the positive class.\n\nNote that this metric is similar to categorical cross entropy widely used in classification tasks. Thus, we don't need to implement the loss from scratch. As the Keras library already has an implementation of this metric, we will simply use the metric to monitor performance of our model.\n","metadata":{}},{"cell_type":"code","source":"log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\", label_smoothing=0.1, from_logits=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:58.739147Z","iopub.execute_input":"2024-07-01T05:46:58.739411Z","iopub.status.idle":"2024-07-01T05:46:58.760424Z","shell.execute_reply.started":"2024-07-01T05:46:58.739387Z","shell.execute_reply":"2024-07-01T05:46:58.759532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dropout\n\nwith strategy.scope():\n\n    # å°†æ‰€æœ‰è¾“å…¥å±‚æ•´åˆåˆ°ä¸€ä¸ªå­—å…¸ä¸­\n    inputs = {\n        \"token_ids\": keras.layers.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),\n        \"padding_mask\": keras.layers.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"),\n        \"features_a\": keras.layers.Input(shape=(14,), name=\"features_a\", dtype=tf.float32),\n        \"features_b\": keras.layers.Input(shape=(14,), name=\"features_b\", dtype=tf.float32),\n    }\n    \n    # Create a DebertaV3Classifier backbone\n    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n        CFG.preset,\n    )\n\n   # ä¿®æ”¹ response_a å’Œ response_b çš„åˆ›å»ºæ–¹å¼ï¼ŒåŒ…å« padding_mask\n    response_a = {\n        \"token_ids\": inputs[\"token_ids\"][:, 0, :],\n        \"padding_mask\": inputs[\"padding_mask\"][:, 0, :]\n    }\n    embed_a = backbone(response_a)\n\n    response_b = {\n        \"token_ids\": inputs[\"token_ids\"][:, 1, :],\n        \"padding_mask\": inputs[\"padding_mask\"][:, 1, :]\n    }\n    embed_b = backbone(response_b)\n    \n    # å°†æ•°å€¼ç‰¹å¾åµŒå…¥\n    len_features_a_embedding = keras.layers.Dense(512, activation='relu')(inputs[\"features_a\"])\n    len_features_b_embedding = keras.layers.Dense(512, activation='relu')(inputs[\"features_b\"])\n    \n    # ä½¿ç”¨ Flatten å±‚å°†æ•°å€¼ç‰¹å¾åµŒå…¥å±•å¹³ä¸ºäºŒç»´å¼ é‡\n    flattened_len_features_a = keras.layers.Flatten()(len_features_a_embedding)\n    flattened_len_features_b = keras.layers.Flatten()(len_features_b_embedding)\n    \n    embed_a = keras.layers.GlobalAveragePooling1D()(embed_a)\n    embed_b = keras.layers.GlobalAveragePooling1D()(embed_b)\n    embeds_text_features_a = keras.layers.Concatenate(axis=-1)([embed_a, flattened_len_features_a])\n    embeds_text_features_b = keras.layers.Concatenate(axis=-1)([embed_b, flattened_len_features_b])\n    \n    # åˆå¹¶æ–‡æœ¬åµŒå…¥å’Œæ•°å€¼ç‰¹å¾åµŒå…¥\n    combined_embeds = keras.layers.Concatenate(axis=-1)([embeds_text_features_a, embeds_text_features_a])\n    \n    # æ·»åŠ L2æ­£åˆ™åŒ–å’ŒDropoutåˆ°æ¨¡å‹ä¸­\n    combined_embeds = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(combined_embeds)  # L2æ­£åˆ™åŒ–\n    combined_embeds = Dropout(0.5)(combined_embeds)  # Dropoutå±‚ï¼Œä¸¢å¼ƒ50%çš„ç¥ç»å…ƒ\n    \n    # å®šä¹‰ temperature_scale å‡½æ•°\n    def temperature_scale(logits, T=1.0):\n        return logits / T\n    \n    # å®šä¹‰æ¸©åº¦å‚æ•° T\n    T = 0.85\n    # åº”ç”¨æ¸©åº¦ç¼©æ”¾\n    scaled_logits = temperature_scale(combined_embeds, T)\n    outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(scaled_logits)\n    \n    model = keras.Model(inputs,  outputs)\n    \n    # Compile the model with optimizer, loss, and metrics\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-6, clipnorm=1.0),\n        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1, from_logits=False),\n        metrics=[\n            log_loss,\n            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n        ],\n    )\n    \n    # æ·»åŠ  AWP å›è°ƒåˆ°æ¨¡å‹è®­ç»ƒä¸­\n    awp_cb = AWPCallback(epsilon=1e-4)  # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ epsilon çš„å€¼","metadata":{"execution":{"iopub.status.busy":"2024-07-01T05:46:58.762014Z","iopub.execute_input":"2024-07-01T05:46:58.762587Z","iopub.status.idle":"2024-07-01T05:47:06.187392Z","shell.execute_reply.started":"2024-07-01T05:46:58.762554Z","shell.execute_reply":"2024-07-01T05:47:06.186599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Summary","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-01T05:47:06.188476Z","iopub.execute_input":"2024-07-01T05:47:06.188774Z","iopub.status.idle":"2024-07-01T05:47:06.233585Z","shell.execute_reply.started":"2024-07-01T05:47:06.188747Z","shell.execute_reply":"2024-07-01T05:47:06.232747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# try:\n#     history = model.fit(\n#         train_ds,\n#         epochs=CFG.epochs,\n#         validation_data=valid_ds,\n#         callbacks=[lr_cb, ckpt_cb]\n#     )\n# except tf.errors.InvalidArgumentError as e:\n#     print(f\"å‡ºç°æ— æ•ˆå‚æ•°é”™è¯¯ï¼š{e}\")\ntry:\n    history = model.fit(\n        train_ds,\n        epochs=CFG.epochs,\n        validation_data=valid_ds,\n        callbacks=[lr_cb, ckpt_cb, awp_cb]  # å°† AWP å›è°ƒæ·»åŠ åˆ°è®­ç»ƒå›è°ƒåˆ—è¡¨ä¸­\n    )\nexcept tf.errors.InvalidArgumentError as e:\n    print(f\"å‡ºç°æ— æ•ˆå‚æ•°é”™è¯¯ï¼š{e}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-01T05:47:06.234698Z","iopub.execute_input":"2024-07-01T05:47:06.234990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Best Model\n\nAfter training, let's load the weight with best result to get the best performance.","metadata":{}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/best_model.weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# # ä½¿ç”¨ FGM æ‰°åŠ¨çš„æ•°æ®é›†è¯„ä¼°æ¨¡å‹\n# fgm_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b,\n#                                      is_fgm=True, epsilon=1.0)\n# evaluation_results = model.evaluate(fgm_ds)\n\n# print(f\"Evaluation results on FGM perturbed dataset: {evaluation_results}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_features_processor = DataFrameStatsProcessor(test_df)\ntest_df_features_a, test_df_features_b = test_df_features_processor.process_dataframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts = test_df.options.tolist()\ntest_ds = build_dataset_with_features(test_texts, features_a=test_df_features_a, features_b=test_df_features_b,\n                         batch_size=min(len(test_df), CFG.batch_size),\n                         shuffle=False)\nprint(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = model.predict(test_ds, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nFollowing code will prepare the submission file.","metadata":{}},{"cell_type":"code","source":"sub_df = test_df[[\"id\"]].copy()\nsub_df[CFG.class_names] = test_preds.tolist()\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}