{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8820093,"sourceType":"datasetVersion","datasetId":5306138},{"sourceId":8826860,"sourceType":"datasetVersion","datasetId":5310688},{"sourceId":8826894,"sourceType":"datasetVersion","datasetId":5310716},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"jax\" or \"torch\"\nimport re\n\nimport keras_nlp\nimport keras\nimport tensorflow as tf\n\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nimport json","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-06T07:10:55.752837Z","iopub.execute_input":"2024-07-06T07:10:55.753177Z","iopub.status.idle":"2024-07-06T07:10:55.757544Z","shell.execute_reply.started":"2024-07-06T07:10:55.753149Z","shell.execute_reply":"2024-07-06T07:10:55.756853Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Num GPUs Available","metadata":{}},{"cell_type":"code","source":"# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n# strategy = tf.distribute.MirroredStrategy()\n# print('Number of devices: {}'.format(strategy.num_replicas_in_sync))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:55.758864Z","iopub.execute_input":"2024-07-06T07:10:55.759213Z","iopub.status.idle":"2024-07-06T07:10:59.826609Z","shell.execute_reply.started":"2024-07-06T07:10:55.759184Z","shell.execute_reply":"2024-07-06T07:10:59.825825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPU","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 42  # Random seed\n    preset = \"deberta_v3_extra_small_en\"\n    sequence_length = 1024\n    epochs = 6\n#     batch_size = 16\n    batch_size = 16 * strategy.num_replicas_in_sync\n    scheduler = 'cosine'  # Learning rate scheduler\n    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n    name2label = {v:k for k, v in label2name.items()}\n    class_labels = list(label2name.keys())\n    class_names = list(label2name.values())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-06T07:10:59.827472Z","iopub.execute_input":"2024-07-06T07:10:59.827724Z","iopub.status.idle":"2024-07-06T07:10:59.832308Z","shell.execute_reply.started":"2024-07-06T07:10:59.827698Z","shell.execute_reply":"2024-07-06T07:10:59.831668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reproducibility \n设置随机种子的值以在每次运行中产生类似的结果。","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:59.833743Z","iopub.execute_input":"2024-07-06T07:10:59.833992Z","iopub.status.idle":"2024-07-06T07:10:59.844228Z","shell.execute_reply.started":"2024-07-06T07:10:59.833967Z","shell.execute_reply":"2024-07-06T07:10:59.843577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Mixed Precision\n\n在本笔记中，我们将使用混合精度而不是 float32 精度进行训练和推理，以减少 GPU 内存使用量。这最终将使我们能够使用更大的批量大小，从而减少我们的训练和推理时间。","metadata":{}},{"cell_type":"code","source":"keras.mixed_precision.set_global_policy(\"mixed_float16\")\n#在mixed_float16策略下，模型的某些部分会自动使用float16进行计算，而其他部分（如损失函数的计算）则可能仍然使用float32以保持稳定性。","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:59.845091Z","iopub.execute_input":"2024-07-06T07:10:59.845340Z","iopub.status.idle":"2024-07-06T07:10:59.854553Z","shell.execute_reply.started":"2024-07-06T07:10:59.845315Z","shell.execute_reply":"2024-07-06T07:10:59.853831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Path ","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:59.855411Z","iopub.execute_input":"2024-07-06T07:10:59.855662Z","iopub.status.idle":"2024-07-06T07:10:59.873018Z","shell.execute_reply.started":"2024-07-06T07:10:59.855638Z","shell.execute_reply":"2024-07-06T07:10:59.872366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta Data \n## Files\n\n### `train.csv`\n- `id`: Unique identifier for each row.\n- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).\n\n### `test.csv`\n- `id`: Unique identifier for each row.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.","metadata":{}},{"cell_type":"code","source":"# Load Train Data\ndf = pd.read_csv(f'{BASE_PATH}/train.csv') \nultrachat_df = pd.read_csv('/kaggle/input/ultrachat-train/ultrachat_s42_a0.5.csv')\ndf = pd.concat([df, ultrachat_df], axis=0)\nlmsys_33k_deduplicated = pd.read_csv('/kaggle/input/lmsys-33k-deduplicated/lmsys-33k-deduplicated.csv')\ndf = pd.concat([df, lmsys_33k_deduplicated], axis=0)\n# ultrafeedback_lmsysformat = pd.read_parquet('/kaggle/input/ultrafeedback-lmsysformat/ultrafeedback_lmsysformat.parquet', engine='pyarrow')\n# ultrafeedback_lmsysformat['prompt'] = ultrafeedback_lmsysformat['prompt'].apply(lambda x: f'[\"{x}\"]')\n# df = pd.concat([df, ultrafeedback_lmsysformat], axis=0)\n\n# Load Test Data\ntest_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n\n# display(ultrafeedback_lmsysformat.head())\ndisplay(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:59.873932Z","iopub.execute_input":"2024-07-06T07:10:59.874159Z","iopub.status.idle":"2024-07-06T07:11:07.428467Z","shell.execute_reply.started":"2024-07-06T07:10:59.874135Z","shell.execute_reply":"2024-07-06T07:11:07.427721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(\"id\", axis=1)\ndf = df.drop_duplicates(keep=\"first\", ignore_index=True)\n\nfor col in [\"prompt\"]:\n    df[col] = df[col].apply(lambda x: eval(x))\n    test_df[col] = test_df[col].apply(lambda x: eval(x))\nfor col in [\"response_a\", \"response_b\"]:\n    df[col] = df[col].apply(lambda x: eval(x.replace(\"null\", \"None\")))\n    test_df[col] = test_df[col].apply(lambda x: eval(x.replace(\"null\", \"None\")))\n    \n# Sample data\n# df = df.sample(frac=0.01)\n\n# Label conversion\ndf[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\ndf[\"class_label\"] = df.class_name.map(CFG.name2label)\n\n# Show Sample\ndisplay(df.head())\n# Show Sample\ndisplay(test_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:11:07.429447Z","iopub.execute_input":"2024-07-06T07:11:07.429736Z","iopub.status.idle":"2024-07-06T07:11:16.257278Z","shell.execute_reply.started":"2024-07-06T07:11:07.429705Z","shell.execute_reply":"2024-07-06T07:11:16.256455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Contextualize Response with Prompt\n\n在我们的方法中，我们将根据提示对每个回答进行情境化，而不是对所有回答使用单一提示。这意味着，对于每个回答，我们将为模型提供同一组提示及其各自的回答（例如，“(P + R_A)”，“(P + R_B)”等）。\n\n> 某些提示和响应可能未使用 `utf-8` 编码，导致创建数据加载器时出错。在这种情况下，我们将用空字符串替换它们。","metadata":{}},{"cell_type":"code","source":"def make_pairs(row):\n    row['options'] = []\n    row[\"encode_fail\"] = False\n\n    try:\n        # 确保所有需要的键都存在于row字典中\n        prompts = row['prompt']\n        responses_a = row['response_a']\n        responses_b = row['response_b']\n        \n        # 检查列表长度是否匹配\n        if not (len(prompts) == len(responses_a) == len(responses_b)):\n            raise ValueError(\"The lists 'prompt', 'response_a', and 'response_b' must be of the same length.\")\n            \n        response_a_str = ''\n        response_b_str = ''\n        \n        for idx in range(len(prompts)):\n            response_a_str += f\"Prompt: {prompts[idx]}\\n\\nResponse: {responses_a[idx]}\"\n            response_b_str += f\"Prompt: {prompts[idx]}\\n\\nResponse: {responses_b[idx]}\"\n        \n        # 文本清洗，例如去除无法识别的Unicode字符或替换它们\n        clean_response_a_str = \"\".join(filter(lambda x: ord(x) < 128, response_a_str))\n        clean_response_b_str = \"\".join(filter(lambda x: ord(x) < 128, response_b_str))\n        \n        row['options'].append(clean_response_a_str)\n        row['options'].append(clean_response_b_str)\n        \n    except KeyError as e:\n        print(f\"Missing key in row: {e}\")\n        row[\"encode_fail\"] = True\n    except ValueError as e:\n        print(e)\n        row[\"encode_fail\"] = True\n    except Exception as e:\n        # 捕获其他所有异常\n        print(f\"An unexpected error occurred: {e}\")\n        row[\"encode_fail\"] = True\n\n    return row","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:11:16.258228Z","iopub.execute_input":"2024-07-06T07:11:16.258506Z","iopub.status.idle":"2024-07-06T07:11:16.266075Z","shell.execute_reply.started":"2024-07-06T07:11:16.258479Z","shell.execute_reply":"2024-07-06T07:11:16.265424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.apply(make_pairs, axis=1)\ndisplay(df.head(2))\n\ntest_df = test_df.apply(make_pairs, axis=1)\ndisplay(test_df.head(2))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:11:16.268591Z","iopub.execute_input":"2024-07-06T07:11:16.268896Z","iopub.status.idle":"2024-07-06T07:13:54.779111Z","shell.execute_reply.started":"2024-07-06T07:11:16.268868Z","shell.execute_reply":"2024-07-06T07:13:54.778342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding Fail Statistics","metadata":{}},{"cell_type":"code","source":"df.encode_fail.value_counts(normalize=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:54.780185Z","iopub.execute_input":"2024-07-06T07:13:54.780465Z","iopub.status.idle":"2024-07-06T07:13:54.787527Z","shell.execute_reply.started":"2024-07-06T07:13:54.780438Z","shell.execute_reply":"2024-07-06T07:13:54.786831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"class DataFrameStatsProcessor:\n    def __init__(self, df):\n        self.df = df\n\n    def _is_empty(self, string: str) -> bool:\n        return bool(re.match(\"^\\s*$\", string))\n\n    def _len(self, string: str) -> int:\n        if string is None:\n            return 0\n        return len(string)\n\n    def _add_len_stats(self, col: str) -> pd.DataFrame:\n        if col == \"prompt\":\n            col_prefix = \"p_len\"\n        elif col == \"response_a\":\n            col_prefix = \"res_a_len\"\n        elif col == \"response_b\":\n            col_prefix = \"res_b_len\"\n        \n        self.df[f\"{col_prefix}_sum\"] = self.df[col].apply(lambda x: sum(self._len(s) for s in x))\n        self.df[f\"{col_prefix}_mean\"] =  self.df[col].apply(lambda x: np.mean(list(self._len(s) for s in x)))\n        self.df[f\"{col_prefix}_max\"] = self.df[col].apply(lambda x: max(self._len(s) for s in x))\n        self.df[f\"{col_prefix}_sum_log\"] = np.log1p(self.df[f\"{col_prefix}_sum\"])\n        self.df[f\"{col_prefix}_mean_log\"] =  np.log1p(self.df[f\"{col_prefix}_mean\"])\n        self.df[f\"{col_prefix}_max_log\"] = np.log1p(self.df[f\"{col_prefix}_max\"])\n        \n        return self.df\n    \n    def z_score_normalize(self, columns):\n        \"\"\"\n        对指定的列进行Z得分归一化。\n        参数:\n            columns (list): 需要进行Z得分归一化的列名列表。\n        \"\"\"\n        for col in columns:\n            self.df[col] = (self.df[col] - self.df[col].mean()) / self.df[col].std()\n    \n    def process_dataframe(self):\n        self.df[\"n_prompts\"] = self.df[\"prompt\"].apply(lambda x: len(x))\n        self.df[\"n_res_a\"] = self.df[\"response_a\"].apply(lambda x: len(x))\n        self.df[\"n_res_b\"] = self.df[\"response_b\"].apply(lambda x: len(x))\n        assert ((self.df[\"n_prompts\"] == self.df[\"n_res_a\"]) & (self.df[\"n_prompts\"] == self.df[\"n_res_b\"])).all()\n\n        self.df[\"n_na_prompts\"] = self.df[\"prompt\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n        self.df[\"n_empty_prompts\"] = self.df[\"prompt\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n        self.df[\"n_na_res_a\"] = self.df[\"response_a\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n        self.df[\"n_empty_res_a\"] = self.df[\"response_a\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n        self.df[\"n_na_res_b\"] = self.df[\"response_b\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n        self.df[\"n_empty_res_b\"] = self.df[\"response_b\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n\n        self.df[\"n_miss_res_a\"] = self.df[\"n_na_res_a\"] + self.df[\"n_empty_res_a\"]\n        self.df[\"n_miss_res_b\"] = self.df[\"n_na_res_b\"] + self.df[\"n_empty_res_b\"]\n\n        self.df[\"n_eff_res_a\"] = self.df[\"n_res_a\"] - self.df[\"n_miss_res_a\"]\n        self.df[\"n_eff_res_b\"] = self.df[\"n_res_b\"] - self.df[\"n_miss_res_b\"]\n\n        self._add_len_stats(\"prompt\")\n        self._add_len_stats(\"response_a\")\n        self._add_len_stats(\"response_b\")\n\n        self.df[\"res_len_mean_diff\"] = self.df[\"res_a_len_mean\"] - self.df[\"res_b_len_mean\"]\n        self.df[\"res_len_mean_diff_clip\"] = self.df[\"res_len_mean_diff\"].clip(-6000, 6000)\n\n        self.df[\"n_miss_prompts\"] = self.df[\"n_na_prompts\"] + self.df[\"n_empty_prompts\"]\n        self.df[\"n_eff_prompts\"] = self.df[\"n_prompts\"] - self.df[\"n_miss_prompts\"]\n\n        self.df[\"na_prompt_ratio\"] = self.df[\"n_na_prompts\"] / self.df[\"n_prompts\"]\n        self.df[\"empty_prompt_ratio\"] = self.df[\"n_empty_prompts\"] / self.df[\"n_prompts\"]\n        self.df[\"miss_prompt_ratio\"] = self.df[\"n_miss_prompts\"] / self.df[\"n_prompts\"]\n\n        self.df[\"na_res_a_ratio\"] = self.df[\"n_na_res_a\"] / self.df[\"n_res_a\"]\n        self.df[\"empty_res_a_ratio\"] = self.df[\"n_empty_res_a\"] / self.df[\"n_res_a\"]\n        self.df[\"miss_res_a_ratio\"] = self.df[\"n_miss_res_a\"] / self.df[\"n_res_a\"]\n        self.df[\"na_res_b_ratio\"] = self.df[\"n_na_res_b\"] / self.df[\"n_res_b\"]\n        self.df[\"empty_res_b_ratio\"] = self.df[\"n_empty_res_b\"] / self.df[\"n_res_b\"]\n        self.df[\"miss_res_b_ratio\"] = self.df[\"n_miss_res_b\"] / self.df[\"n_res_b\"]\n\n        for col, col_prefix in zip([\"prompt\", \"response_a\", \"response_b\"], [\"p_len\", \"res_a_len\", \"res_b_len\"]):\n            self.df[f\"{col_prefix}_med\"] = self.df[col].apply(lambda x: np.median(list(self._len(s) for s in x)))\n            self.df[f\"{col_prefix}_std\"] = self.df[col].apply(lambda x: np.std(list(self._len(s) for s in x)))\n\n        self.df[\"p_len_eff_mean\"] = self.df[\"p_len_sum\"] / self.df[\"n_eff_prompts\"]\n        self.df[\"res_a_len_eff_mean\"] = self.df[\"res_a_len_sum\"] / self.df[\"n_eff_res_a\"]\n        self.df[\"res_b_len_eff_mean\"] = self.df[\"res_b_len_sum\"] / self.df[\"n_eff_res_b\"]\n\n        for stats in [\"sum\", \"mean\", \"max\", \"med\", \"eff_mean\"]:\n            self.df[f\"p_a_{stats}_diff\"] = self.df[f\"p_len_{stats}\"] - self.df[f\"res_a_len_{stats}\"]\n            self.df[f\"p_b_{stats}_diff\"] = self.df[f\"p_len_{stats}\"] - self.df[f\"res_b_len_{stats}\"]\n            self.df[f\"a_b_{stats}_diff\"] = self.df[f\"res_a_len_{stats}\"] - self.df[f\"res_b_len_{stats}\"]\n            \n        len_feature_a_col = [\"res_a_len_sum\",\"res_a_len_mean\",\"res_a_len_max\",\"res_a_len_sum_log\",\"res_a_len_mean_log\",\"res_a_len_max_log\",\n                     \"res_a_len_med\",\"res_a_len_std\",\"res_a_len_eff_mean\",\"p_a_sum_diff\",\"p_a_mean_diff\",\"p_a_max_diff\",\"p_a_med_diff\",\n                     \"p_a_eff_mean_diff\"]\n        \n        len_feature_b_col = [\"res_b_len_sum\",\"res_b_len_mean\",\"res_b_len_max\",\"res_b_len_sum_log\",\"res_b_len_mean_log\",\"res_b_len_max_log\",\n                             \"res_b_len_med\",\"res_b_len_std\",\"res_b_len_eff_mean\",\"p_b_sum_diff\",\"p_b_mean_diff\",\"p_b_max_diff\",\"p_b_med_diff\",\n                             \"p_b_eff_mean_diff\"]\n        \n        numerical_feature_columns = [\"res_a_len_sum\",\"res_a_len_mean\",\"res_a_len_max\",\"res_a_len_sum_log\",\"res_a_len_mean_log\",\"res_a_len_max_log\",\n                                     \"res_a_len_med\",\"res_a_len_std\",\"res_a_len_eff_mean\",\"p_a_sum_diff\",\"p_a_mean_diff\",\"p_a_max_diff\",\"p_a_med_diff\",\n                                     \"p_a_eff_mean_diff\", \"res_b_len_sum\",\"res_b_len_mean\",\"res_b_len_max\",\"res_b_len_sum_log\",\"res_b_len_mean_log\",\"res_b_len_max_log\",\n                                     \"res_b_len_med\",\"res_b_len_std\",\"res_b_len_eff_mean\",\"p_b_sum_diff\",\"p_b_mean_diff\",\"p_b_max_diff\",\"p_b_med_diff\",\n                                     \"p_b_eff_mean_diff\"]\n        # 确保不除以零进行归一化\n        for col in numerical_feature_columns:\n            if self.df[col].std() == 0:\n                print(f\"Warning: Standard deviation is zero for column {col}. Skipping normalization.\")\n            else:\n                self.z_score_normalize([col])\n                \n        self.df = self.df.fillna(0)\n        \n        # 选择这些列并将它们转换为列表\n        len_features_a = self.df[len_feature_a_col].values.tolist()\n        len_features_b = self.df[len_feature_b_col].values.tolist()\n\n        return len_features_a, len_features_b","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:54.788456Z","iopub.execute_input":"2024-07-06T07:13:54.788710Z","iopub.status.idle":"2024-07-06T07:13:54.833591Z","shell.execute_reply.started":"2024-07-06T07:13:54.788684Z","shell.execute_reply":"2024-07-06T07:13:54.832923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Split\n\n在下面提供的代码片段中，我们将使用class_label列的分层将现有数据分为训练和验证。","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # Import package\n\ntrain_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"class_label\"])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-06T07:13:54.834446Z","iopub.execute_input":"2024-07-06T07:13:54.834693Z","iopub.status.idle":"2024-07-06T07:13:56.025187Z","shell.execute_reply.started":"2024-07-06T07:13:54.834668Z","shell.execute_reply":"2024-07-06T07:13:56.024147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n    preset=CFG.preset, \n    sequence_length=CFG.sequence_length, \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:56.026330Z","iopub.execute_input":"2024-07-06T07:13:56.026880Z","iopub.status.idle":"2024-07-06T07:13:59.619397Z","shell.execute_reply.started":"2024-07-06T07:13:56.026847Z","shell.execute_reply":"2024-07-06T07:13:59.618308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_fn(text, label=None, features_a=None, features_b=None):\n    text = preprocessor(text)\n    if features_a is not None:\n        text['features_a'] = features_a\n    if features_b is not None:\n         text['features_b'] = features_b\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:59.620555Z","iopub.execute_input":"2024-07-06T07:13:59.620834Z","iopub.status.idle":"2024-07-06T07:13:59.625837Z","shell.execute_reply.started":"2024-07-06T07:13:59.620807Z","shell.execute_reply":"2024-07-06T07:13:59.624970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FGM","metadata":{}},{"cell_type":"code","source":"# # 添加 FGM 扰动函数\n# def fgm_perturb(features, epsilon=1.0):\n#     # 计算扰动量，epsilon 为扰动比例\n#     perturbation = np.random.uniform(-1, 1, features.shape) * epsilon\n#     # 应用扰动\n#     return features + perturbation","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:59.626781Z","iopub.execute_input":"2024-07-06T07:13:59.627022Z","iopub.status.idle":"2024-07-06T07:13:59.637652Z","shell.execute_reply.started":"2024-07-06T07:13:59.627000Z","shell.execute_reply":"2024-07-06T07:13:59.636936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 修改数据预处理函数以包含 FGM 扰动\n# def preprocess_fn(text, label=None, features_a=None, features_b=None, is_fgm=False, epsilon=1.0):\n#     # 预处理文本\n#     text = preprocessor(text)\n#     if features_a is not None:\n#         if is_fgm:\n#             # 如果是 FGM，应用扰动\n#             features_a = fgm_perturb(features_a, epsilon)\n#         text['features_a'] = features_a\n#     if features_b is not None:\n#         if is_fgm:\n#             # 如果是 FGM，应用扰动\n#             features_b = fgm_perturb(features_b, epsilon)\n#         text['features_b'] = features_b\n#     return (text, label) if label is not None else text","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:59.638528Z","iopub.execute_input":"2024-07-06T07:13:59.638751Z","iopub.status.idle":"2024-07-06T07:13:59.649592Z","shell.execute_reply.started":"2024-07-06T07:13:59.638728Z","shell.execute_reply":"2024-07-06T07:13:59.648820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AWP","metadata":{}},{"cell_type":"code","source":"#定义 AWP 扰动函数\ndef awp_perturb(model, epsilon=1e-4):\n    for layer in model.layers:\n        if hasattr(layer, 'kernel'):\n            # 获取权重\n            weights = layer.kernel\n            # 计算扰动\n            perturbation = tf.random.normal(weights.shape, stddev=epsilon)\n            # 应用扰动\n            layer.kernel.assign_add(perturbation)\n\n#创建 AWP 回调函数\nclass AWPCallback(keras.callbacks.Callback):\n    def __init__(self, epsilon):\n        super(AWPCallback, self).__init__()\n        self.epsilon = epsilon\n\n    def on_batch_begin(self, batch, logs=None):\n        # 在每个批次开始时应用 AWP 扰动\n        awp_perturb(self.model, self.epsilon)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:59.650602Z","iopub.execute_input":"2024-07-06T07:13:59.650984Z","iopub.status.idle":"2024-07-06T07:13:59.677438Z","shell.execute_reply.started":"2024-07-06T07:13:59.650956Z","shell.execute_reply":"2024-07-06T07:13:59.676557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader\n\n下面的代码使用tf.data.Dataset为数据处理设置了一个健壮的数据流管道。","metadata":{}},{"cell_type":"code","source":"def build_dataset_with_features(texts, labels=None, features_a=None, features_b=None, batch_size=32, is_fgm=False,  epsilon=1.0,\n                                cache=True, shuffle=1024):\n    AUTO = tf.data.AUTOTUNE\n    if (features_a is not None) and (features_b is not None):\n        slices = (texts, None, features_a, features_b) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3), features_a, features_b)  # Create slices\n    else:\n        slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.cache() if cache else ds\n    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)\n#     ds = ds.map(lambda x: preprocess_fn(x, features_a=features_a, features_b=features_b, is_fgm=is_fgm, epsilon=epsilon),\n#                 num_parallel_calls=tf.data.AUTOTUNE)\n    opt = tf.data.Options()\n    if shuffle:\n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)\n    ds = ds.batch(batch_size, drop_remainder=False)\n    ds = ds.prefetch(AUTO)\n    \n    return ds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-06T07:13:59.678498Z","iopub.execute_input":"2024-07-06T07:13:59.679456Z","iopub.status.idle":"2024-07-06T07:13:59.690360Z","shell.execute_reply.started":"2024-07-06T07:13:59.679392Z","shell.execute_reply":"2024-07-06T07:13:59.689444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Train/Valid Dataloader","metadata":{}},{"cell_type":"code","source":"train_features_processor = DataFrameStatsProcessor(train_df.copy())\ntrain_features_a, train_features_b = train_features_processor.process_dataframe()\nvalid_features_processor = DataFrameStatsProcessor(valid_df.copy())\nvalid_features_a, valid_features_b = valid_features_processor.process_dataframe()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:13:59.691503Z","iopub.execute_input":"2024-07-06T07:13:59.691767Z","iopub.status.idle":"2024-07-06T07:14:20.001979Z","shell.execute_reply.started":"2024-07-06T07:13:59.691741Z","shell.execute_reply":"2024-07-06T07:14:20.000863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train\ntrain_texts = train_df.options.tolist()  \ntrain_labels = train_df.class_label.tolist() \ntrain_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b, \n                         batch_size=CFG.batch_size,\n                         shuffle=True)\n# # Valid\nvalid_texts = valid_df.options.tolist()  \nvalid_labels = valid_df.class_label.tolist() \nvalid_ds = build_dataset_with_features(valid_texts, valid_labels, valid_features_a, valid_features_b, \n                         batch_size=CFG.batch_size,\n                         shuffle=False)\nprint(train_ds)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-07-06T07:14:20.003044Z","iopub.execute_input":"2024-07-06T07:14:20.003310Z","iopub.status.idle":"2024-07-06T07:14:35.797076Z","shell.execute_reply.started":"2024-07-06T07:14:20.003284Z","shell.execute_reply":"2024-07-06T07:14:35.796277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LR Schedule\n\n实施学习率调度程序对于迁移学习至关重要。\n\n学习率从 lr_start 开始，然后使用各种技术逐渐减小到 lr_min，包括：\n\n- step：以类似楼梯的方式逐步降低学习率。\n- cos：利用余弦曲线逐渐降低学习率。\n- exp：以指数方式降低学习率。\n\n**重要性**：结构良好的学习率调度对于有效的模型训练至关重要，可确保最佳收敛并避免诸如过冲或停滞等问题。","metadata":{}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10):\n    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n    \n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-06T07:14:35.798130Z","iopub.execute_input":"2024-07-06T07:14:35.798406Z","iopub.status.idle":"2024-07-06T07:14:35.804899Z","shell.execute_reply.started":"2024-07-06T07:14:35.798377Z","shell.execute_reply":"2024-07-06T07:14:35.804119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.batch_size, epochs=CFG.epochs)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:14:35.805677Z","iopub.execute_input":"2024-07-06T07:14:35.805922Z","iopub.status.idle":"2024-07-06T07:14:35.817489Z","shell.execute_reply.started":"2024-07-06T07:14:35.805898Z","shell.execute_reply":"2024-07-06T07:14:35.816783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Checkpointing\n\n下面的代码将创建一个回调，在训练期间保存模型的最佳检查点，我们将在提交时使用它进行推理。","metadata":{}},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',\n                                          monitor='val_log_loss',\n                                          save_best_only=True,\n                                          save_weights_only=True,\n                                          mode='min')  # Get Model checkpoint callback","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-06T07:14:35.818314Z","iopub.execute_input":"2024-07-06T07:14:35.818564Z","iopub.status.idle":"2024-07-06T07:14:35.829356Z","shell.execute_reply.started":"2024-07-06T07:14:35.818538Z","shell.execute_reply":"2024-07-06T07:14:35.828631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric\n\n这次比赛的指标是对数损失。这个度量可以用数学表示为：\n\n$$\n\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)\n$$","metadata":{}},{"cell_type":"code","source":"log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\", label_smoothing=0.1, from_logits=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:14:35.830220Z","iopub.execute_input":"2024-07-06T07:14:35.830459Z","iopub.status.idle":"2024-07-06T07:14:35.843707Z","shell.execute_reply.started":"2024-07-06T07:14:35.830435Z","shell.execute_reply":"2024-07-06T07:14:35.843070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dropout\n\nwith strategy.scope():\n\n    # 将所有输入层整合到一个字典中\n    inputs = {\n        \"token_ids\": keras.layers.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),\n        \"padding_mask\": keras.layers.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"),\n        \"features_a\": keras.layers.Input(shape=(14,), name=\"features_a\", dtype=tf.float32),\n        \"features_b\": keras.layers.Input(shape=(14,), name=\"features_b\", dtype=tf.float32),\n    }\n    \n    # Create a DebertaV3Classifier backbone\n    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n        CFG.preset,\n    )\n\n   # 修改 response_a 和 response_b 的创建方式，包含 padding_mask\n    response_a = {\n        \"token_ids\": inputs[\"token_ids\"][:, 0, :],\n        \"padding_mask\": inputs[\"padding_mask\"][:, 0, :]\n    }\n    embed_a = backbone(response_a)\n\n    response_b = {\n        \"token_ids\": inputs[\"token_ids\"][:, 1, :],\n        \"padding_mask\": inputs[\"padding_mask\"][:, 1, :]\n    }\n    embed_b = backbone(response_b)\n    \n    # 将数值特征嵌入\n    len_features_a_embedding = keras.layers.Dense(512, activation='relu')(inputs[\"features_a\"])\n    len_features_b_embedding = keras.layers.Dense(512, activation='relu')(inputs[\"features_b\"])\n    \n    # 使用 Flatten 层将数值特征嵌入展平为二维张量\n    flattened_len_features_a = keras.layers.Flatten()(len_features_a_embedding)\n    flattened_len_features_b = keras.layers.Flatten()(len_features_b_embedding)\n    \n    embed_a = keras.layers.GlobalAveragePooling1D()(embed_a)\n    embed_b = keras.layers.GlobalAveragePooling1D()(embed_b)\n    embeds_text_features_a = keras.layers.Concatenate(axis=-1)([embed_a, flattened_len_features_a])\n    embeds_text_features_b = keras.layers.Concatenate(axis=-1)([embed_b, flattened_len_features_b])\n    \n    # 合并文本嵌入和数值特征嵌入\n    combined_embeds = keras.layers.Concatenate(axis=-1)([embeds_text_features_a, embeds_text_features_a])\n    \n    # 添加L2正则化和Dropout到模型中\n    combined_embeds = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(combined_embeds)  # L2正则化\n    combined_embeds = Dropout(0.05)(combined_embeds)  # Dropout层，丢弃5%的神经元\n    \n    # 定义 temperature_scale 函数\n    def temperature_scale(logits, T=1.0):\n        return logits / T\n    \n    # 定义温度参数 T\n    T = 0.85\n    # 应用温度缩放\n    scaled_logits = temperature_scale(combined_embeds, T)\n    outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(scaled_logits)\n    \n    model = keras.Model(inputs,  outputs)\n    \n    # Compile the model with optimizer, loss, and metrics\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-6, clipnorm=1.0),\n        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1, from_logits=False),\n        metrics=[\n            log_loss,\n            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n        ],\n    )\n    \n    # 添加 AWP 回调到模型训练中\n    awp_cb = AWPCallback(epsilon=1e-4)  # 您可以根据需要调整 epsilon 的值","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:14:35.844748Z","iopub.execute_input":"2024-07-06T07:14:35.844998Z","iopub.status.idle":"2024-07-06T07:14:42.454315Z","shell.execute_reply.started":"2024-07-06T07:14:35.844974Z","shell.execute_reply":"2024-07-06T07:14:42.453100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Summary","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-06T07:14:42.455463Z","iopub.execute_input":"2024-07-06T07:14:42.455717Z","iopub.status.idle":"2024-07-06T07:14:42.490147Z","shell.execute_reply.started":"2024-07-06T07:14:42.455690Z","shell.execute_reply":"2024-07-06T07:14:42.489318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# try:\n#     history = model.fit(\n#         train_ds,\n#         epochs=CFG.epochs,\n#         validation_data=valid_ds,\n#         callbacks=[lr_cb, ckpt_cb]\n#     )\n# except tf.errors.InvalidArgumentError as e:\n#     print(f\"出现无效参数错误：{e}\")\ntry:\n    history = model.fit(\n        train_ds,\n        epochs=CFG.epochs,\n        validation_data=valid_ds,\n        callbacks=[lr_cb, ckpt_cb, awp_cb]  # 将 AWP 回调添加到训练回调列表中\n    )\nexcept tf.errors.InvalidArgumentError as e:\n    print(f\"出现无效参数错误：{e}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-06T07:14:42.493473Z","iopub.execute_input":"2024-07-06T07:14:42.493718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Best Model","metadata":{}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/best_model.weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# # 使用 FGM 扰动的数据集评估模型\n# fgm_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b,\n#                                      is_fgm=True, epsilon=1.0)\n# evaluation_results = model.evaluate(fgm_ds)\n\n# print(f\"Evaluation results on FGM perturbed dataset: {evaluation_results}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_features_processor = DataFrameStatsProcessor(test_df)\ntest_df_features_a, test_df_features_b = test_df_features_processor.process_dataframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts = test_df.options.tolist()\ntest_ds = build_dataset_with_features(test_texts, features_a=test_df_features_a, features_b=test_df_features_b,\n                         batch_size=min(len(test_df), CFG.batch_size),\n                         shuffle=False)\nprint(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = model.predict(test_ds, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub_df = test_df[[\"id\"]].copy()\nsub_df[CFG.class_names] = test_preds.tolist()\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}